
### *인공지능 1장

#### 마빈 민스키
>인공지능의 개척을 이끈 사람

#### 인공지능 성공의 원동력
- 오픈소스
- 컴퓨팅파워
- 빅데이터

#### 인공지능의 두가지 큰 흐름
>기호주의와 연결주의로 나누어졌다. 초기에는 기호주의가 우세했으나 추후 한계점이 드러나면서 연결주의 인공지능이 지금까지 발전해왔다

#### 기호주의란
>인간의 지식을 기호화 하여 컴퓨터에게 규칙을 모두 설정해두는 인공지능이다.

#### 연결주의란
>사람의 두뇌 뉴런의 연결을 모방하여 제작한 인공지능이다 -> 최초의 인공 신경망 [[퍼셉트론]]

#### 자연어 처리에 대한 기호주의 vs 연결주의
>기호주의는 단어, 문법을 모두 기계에게 하나 씩 입력해줘야한다. 그러나 방대한 자연어의 양을 모두 입력하기에는 불가능에 가깝다. 또한 문맥에 따라 다른 의미로 사용되는 문장, 단어의 의미를 해석할 수 없기에 그 한계점이 드러난다. 이 두가지의 문제를 구문 분석, 의미 분석의 문제이다.
>이를 기호, 규칙이 아닌 통계적으로 처리하기 시작하였다. 다음에 올 문장, 단어를 통계적으로 추론하여 자연어를 처리하는 연결주의가 결국 승리하게 되었다.

#### 기계학습에서 학습이란
>기계학습에서 학습은 적절한 매게변수인 가중치를 기계 스스로 찾는 과정을 의미한다.




### *퍼셉트론

#### 퍼셉트론
>기본적으로 y = x1 * w1 + x2 * w2으로 식을 풀고, 임계값을 넘으면 1을 아니면 0을 출력한다
```python
# 퍼셉트론 함수
def perceptron(x1, x2):
	w1, w2, b = 0.5, 0.5, 0.2
	y = x1*w1 + x2*w2
	if y > b:
		return 1
	elseL
		return 0
```


#### 임계값
> 임계값은 활성화를 위한 기준으로 theta(θ)로 표기하기도 한다.
> 여기서는 theta를 -b로 치환하여 가중치계산과 같이 계산한다.

#### 단층 퍼셉트론
>하나의 층으로만 이루어진 퍼셉트론으로 AND, OR, NAND와 같은 단순한 연산으로 이루어진다.

#### 퍼셉트론의 논리 연산

##### 퍼셉트론 AND
```python
# 퍼셉트론 함수 AND 연산
def perceptron_and(x1, x2):
	w1, w2, b = 0.5, 0.5, -0.7
	y = x1*w1 + x2*w2 + b
	if y > 0:
		return 1
	elseL
		return 0

perceptron_and(0, 0)
perceptron_and(0, 1)
perceptron_and(1, 0)
perceptron_and(1, 1)
```

##### 퍼셉트론 OR
```python
# 퍼셉트론 함수 OR 연산
def perceptron_or(x1, x2):
	w1, w2, b = 0.5, 0.5, -0.2
	y = x1*w1 + x2*w2 + b
	if y > 0:
		return 1
	elseL
		return 0

perceptron_or(0, 0)
perceptron_or(0, 1)
perceptron_or(1, 0)
perceptron_or(1, 1)
```

##### 퍼셉트론 NAND
```python
# 퍼셉트론 함수 NAND 연산
def perceptron_nand(x1, x2):
	w1, w2, b = -0.5, -0.5, 0.7
	y = x1*w1 + x2*w2 + b
	if y > 0:
		return 1
	elseL
		return 0

perceptron_nand(0, 0)
perceptron_nand(0, 1)
perceptron_nand(1, 0)
perceptron_nand(1, 1)
```

#### 단층 퍼셉트론의 한계와 다층 퍼셉트론
>다층 퍼셉트론으로는 XOR같은 복잡한 구조의 연산을 수행할 수 없다
>이를 해결하기 위해 나온 퍼셉트론이 다층 퍼셉트론이다.
>단층은 선형적으로 결과가 나타나기에 복잡한 문제를 풀 수 없으나
>다층은 비선형적 결과가 나타나며, 이는 더욱 복잡하고 어려운 문제를 해결할 수 있다.

#### 다층 퍼셉트론 XOR
```python
# 퍼셉트론 함수 XOR 연산
def perceptron_xor(x1, x2):
	w1, w2, b = -0.5, -0.5, 0.7
	s1 = perceptron_nand(x1, x2)
	s2 = perceptron_or(x1, x2)
	y = s1*w1 + s2*w2 + b
	if y > 0:
		return 1
	else
		return 0

perceptron_xor(0, 0)
perceptron_xor(0, 1)
perceptron_xor(1, 0)
perceptron_xor(1, 1)
```

#### 선형 함수와 비선형 함수
>위에서 언급한 단층과 다층의 차이로 선형함수는 직선의 결과를 비선형 함수는 직선이 아닌 결과를 나타낸다. 딥러닝 학습에서 선형함수를 사용하지 않는 이유는 은닉층이 의미가 없어지기 때문이다. 아무리 은닉층을 늘린다고 하여도 선형적이여서 하나의 식으로 통일되기 때문이다. 이는 복잡한 문제를 해결하는데 있어서 매우 약화된 성능을 보여줄 것이다. 


### *3장 신경망, 활성화 함수

#### 신경망
>퍼셉트론과 신경망의 차이점은 활성화 함수가 있다. 퍼셉트론과 달리 다양한 활성화 함수를 통해 은닉층을 보다 복잡하고 어려운 문제를 해결할 수 있는 구조를 이룰 수 있다. 또한 이런 활성화 함수를 이용하여 학습에 있어서 가중치를 스스로 구할 수 있게 만든다.

#### 활성화 함수 종류

##### 계단 함수
```python
def step(x):
    y = x > 0
    return y.astype(int)
    ```

##### 시그모이드 함수
```python
def sigmoid(x):
    y = 1 / (1 + np.exp(-x))
    return y
```

##### 렐루 함수
```python
def ReLu(x):
    return np.maximum(0, x)
```
#### 활성화 함수를 이용한 3층 신경망

##### 시그모이드 함수
```python
# 1층 -> 2층
x = np.array([1.0, 0.5])
b = np.array([0.1, 0.2, 0.3])
w = np.array([ 
	[0.1, 0.3, 0.5], 
	[0.2, 0.4, 0.6] ])

r1 = np.dot(x, w) + b
z1 = sigmoid(r1)

# 2층 -> 3층
x2 = z1
b2 = np.array([0.1, 0.2])
w2 = np.array([
    [0.1, 0.4],
    [0.2, 0.5],
    [0.3, 0.6] ])

r2 = np.dot(x2, w2) + b2
z2 = sigmoid(a2)

# 3층
x3 = z2
b3 = np.array([0.1, 0.2])
w3 = np.array([
    [0.1, 0.3],
    [0.2, 0.4]
])
y = np.dot(x3, w3) + b3
```

#### 출력층의 함수

##### 항등 함수
```python
def identity(x):
    return x
```

##### 소프트맥스 함수
```python
def softmax(x):
    c = np.max(x)
    exp_x = np.exp(x - c)
    sum_exp_x = np.sum(exp_x)
    y = exp_x / sum_exp_x
    return y
```


### *4장 신경망 학습

#### 미니배치
>미니배치는 전체적인 학습 데이터에서 작은 그룹을 뽑아 그 반복적으로 학습을 하는 과정이다. 학습 데이터에서 골고루 뽑아 범용성있는 성능을 올리는 것이 중요하다. 한쪽만 치우쳐진 학습 결과는 편향적으로 결과를 내기에 다양하게 학습 데이터를 골라내는 것이 중요하다.

#### 미니배치와 손실 함수
>일부 골라낸 데이터의 손실 함수를 구하고, 손실 함수의 합을 통해 평균을 구하면 하나의 지표가 된다.

#### 훈련 데이터와 시험 데이터
>이 두 데이터를 나누는 이유는 범용성을 위해서다. 범용성이란 학습한 데이터가 아닌 새로운 데이터에서도 올바른 추론을 하는 능력으로 학습 이후 시험 데이터를 통해 범용성을 테스트한다.

#### 오버피팅
>학습이 편향되어 범용성이 떨어진 학습을 의미한다

#### 손실 함수
>손실 함수는 학습이 올바른지 검사하는 지표이다. 기본적으로 0에 가까우면 올바른 추론이며, 반대로 멀어진다면 오답에 가까운 추론이다.

#### 손실 함수를 지표로 되는 이유
>정확도를 지표로 삼으면 대부분의 구간에서 미분이 0이 되기에 학습이 안된다.


##### 오차제곱합
```python
def sum_squares_error(y, t):
	return 0.5 * np.sum((y-t)**2)
```

##### 교차 엔트로피 오차
```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```
>1e-7을 y에 더하는 이유는 0이 되어버리면 미분을 할 수 없기 때문이다.

#### 미분의 기능
>학습에 있어 미분은 상당히 중요한 역할을 한다.
>미분이란 한 순간의 변화량을 나타내는데 이는 학습에 결과가 어떤 방향으로 나아갈 지 정하는 수치가 된다. 손실 함수에 따라 올바른 학습의 기울기를 정하는데 미분이 사용된다.

##### 미분
```python
def differential(f, x):
    h = 1e-4
    return (f(x+h)) / (h)
```

##### 수치 미분
> 중앙 차분으로 기존 미분보다 정확한 결과를 보여준다.
```python
def differential(f, x):
    h = 1e-4
    return (f(x+h) - f(x-h)) / (2*h)
```

##### 편미분
>연산에 미지수가 2개 이상일 때 하나의 x값에 초점을 두어 미분을 구한다. 나머지 값은 고정하고, 초점을 둔 x의 변화 따른 변화량만 구하는 작업이다.

##### 기울기
>편미분을 벡터로 정리한 것이 기울기이다.
>각 x에 대해 편미분을 구한 값이 기울기다. 기울기가 의미하는 것은 기울기의 방향이 출력값을 가장 크게 줄이는 방향을 의미한다. 그러나 기울기가 늘 최소값을 가리키는 건 아니나 조금이라도 작은 수를 가리킨다.

##### 경사법
>경사법은 기울기를 통해 학습을 하는 방법이다. 현재 기울기의 어느정도 앞에가고, 그 위치에서 또 기울기를 구하는걸 반복하며 함수의 값을 줄인다.

##### 경사 하강법
>경사법으로 최소값을 찾는 과정이다. 반대로 상승법은 최대값을 찾는다.

#### **전체적인 학습 과정**
1. 학습에 맞게 신경망을 구성하며, 가중치는 랜덤으로, 편향은 0으로 설정한다.
2. 학습 데이터를 랜덤하게 선별하여 미니배치를 준비한다 -> **미니배치**
3. 미니배치 데이터를 신경망에 넣어 계산한다. -> **sigmoid, softmax**
4. 출력값의 손실 함수를 구한다 -> **평균 제곱 오차, 교차 엔트로피 오차**
5. 손실 함수를 바탕으로 수치 미분 or 오차역전파를 수행한다 -> **중앙 차분, 편미분**
6. 기울기를 구하고 가중치를 설정한다 -> **경사하강법, 학습률, 기울기**
7. 2번~6번의 결과를 반복이 끝날 때 까지 수행한다.





### *5장 역전파

#### 수치 미분과 역전파
>수치 미분과 같은 결과를 얻으나 더 효율적이고 빠른 계산이 가능한 것이 역전파이다. 수치미분은 모든 가중치에 대해서 중앙 차분을 하고, 기울기를 구한 후 값을 변경한다.

##### 오차역전파법
>결과의 오차를 역으로 전파하여 가중치를 수정하는 방식

##### 연쇄 법칙
>역전파는 곧 연쇄 법칙이다. 역전파를 하며 각 함수와 계산에 대해 미분의 값을 곱하면 결국 x과 z의 미분을 구할 수 있다.

##### 덧셈의 역전파
>덧셈은 그저 흐르게 둔다

##### 곱셈의 역전파
> 반대 피연산자와 더한 후 흐른다



