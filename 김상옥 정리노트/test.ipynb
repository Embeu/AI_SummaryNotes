{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84110f89-7eba-4be5-a231-2ccb6fa94058",
   "metadata": {},
   "source": [
    "#### 필요 라이브러리, 데이터셋 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c0a67a-694b-4e9d-aece-598d66bbd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ef2723-4b8a-49ad-bbe5-403f5dc18467",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d899b361-d444-4367-b578-c42f75c4f4c8",
   "metadata": {},
   "source": [
    "## --사용 함수--\n",
    "----\n",
    "\n",
    "### 활성화 함수\n",
    "- 시그모이드\n",
    "- 소프트맥스 (크기에 맞게 변경)\n",
    "\n",
    "### 손실 함수\n",
    "- 교차 엔트로피 오차\n",
    "\n",
    "### 기울기\n",
    "- 편미분, 수치 미분 --> 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c365b5ad-4ef6-46ec-aba7-8c5d07fbae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82fd3e3-c283-46d0-b23c-50f8e7fa32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887165da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8e1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(x):\n",
    "    print(x.shape)\n",
    "    grad = np.zeros(x)\n",
    "    grad[x>=0] = 1\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56751348-de2e-4978-85d7-fbf4070a5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f783e8-112a-4634-a622-d2665d1fa9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c86de",
   "metadata": {},
   "source": [
    "# SGD 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428e76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, Ir=0.01):\n",
    "        self.Ir =Ir\n",
    "        print(\"SGD 생성\")\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.Ir * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d5a70",
   "metadata": {},
   "source": [
    "# Momentum 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a5e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        print(\"Momentum 생성\")\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d9d43",
   "metadata": {},
   "source": [
    "# AdaGrad 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71488479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__ (self, Ir=0.01):\n",
    "        self.Ir =Ir\n",
    "        self.h = None\n",
    "        print(\"AdaGrad 생성\")\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys() :\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.Ir * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a90ea5",
   "metadata": {},
   "source": [
    "```python\n",
    "# 기존에 0.01을 곱하여 사용한 가중치 초기화\n",
    "        # 첫 번째 층\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size[0])\n",
    "        self.params['b1'] = np.zeros(hidden_size[0])\n",
    "\n",
    "        # 중간 층\n",
    "        for i in range(1, self.maxIndex - 1):\n",
    "            self.params['W' + str(i + 1)] = weight_init_std * np.random.randn(hidden_size[i - 1], hidden_size[i])\n",
    "            self.params['b' + str(i + 1)] = np.zeros(hidden_size[i])\n",
    "\n",
    "        # 마지막 층\n",
    "        self.params['W' + str(self.maxIndex)] = weight_init_std * np.random.randn(hidden_size[-1], output_size)\n",
    "        self.params['b' + str(self.maxIndex)] = np.zeros(output_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b2229b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_initializer(fan_in, fan_out, gain=np.sqrt(2.0)):\n",
    "    range = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return np.random.randn(fan_in, fan_out) * gain * range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ff26c-4310-44bf-94a6-ddd12a40a2a4",
   "metadata": {},
   "source": [
    "## 학습에 사용될 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c4a932b-99f6-46c9-93b5-26622b478882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    # 가중치 초기화\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.1):\n",
    "        self.params = {}\n",
    "        self.maxIndex = len(hidden_size)  # 입력층, 출력층 포함\n",
    "        \n",
    "        node_num = input_size\n",
    "        \n",
    "        # 첫 번째 층\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size[0]) * weight_init_std\n",
    "        self.params['b1'] = np.zeros(hidden_size[0])\n",
    "\n",
    "        # 중간 층\n",
    "        for i in range(1, self.maxIndex - 1):\n",
    "            node_num = hidden_size[i - 1]\n",
    "            self.params['W' + str(i + 1)] = np.random.randn(hidden_size[i - 1], hidden_size[i]) * weight_init_std\n",
    "            self.params['b' + str(i + 1)] = np.zeros(hidden_size[i])\n",
    "\n",
    "        # 마지막 층\n",
    "        node_num = hidden_size[-1]\n",
    "        self.params['W' + str(self.maxIndex)] = np.random.randn(hidden_size[-1], output_size) * weight_init_std\n",
    "        self.params['b' + str(self.maxIndex)] = np.zeros(output_size)\n",
    "\n",
    "        # 모든 가중치 및 편향 출력 확인\n",
    "        for key in self.params.keys():\n",
    "            print(key)\n",
    "        \n",
    "        \n",
    "    # 가중치와 편향으로 계산 신경망 계산\n",
    "    def predict(self, x):\n",
    "        a, z = {}, {}\n",
    "    \n",
    "        a[\"a1\"] = np.dot(x, self.params['W1']) + self.params['b1']\n",
    "        z[\"z1\"] = sigmoid(a[\"a1\"])\n",
    "        \n",
    "        for i in range(1, self.maxIndex - 1):\n",
    "            a[\"a\"+str(i+1)] = np.dot(z[\"z\"+str(i)], self.params['W'+str(i+1)]) + self.params['b'+str(i+1)]\n",
    "            z[\"z\"+str(i+1)] = sigmoid(a[\"a\"+str(i+1)])\n",
    "        \n",
    "        a[\"a\"+str(self.maxIndex)] = np.dot(z[\"z\"+str(self.maxIndex-1)], self.params['W'+str(self.maxIndex)]) + self.params['b'+str(self.maxIndex)]\n",
    "        y = softmax(a[\"a\"+str(self.maxIndex)])\n",
    "        \n",
    "        return y        \n",
    "        \n",
    "    # 교차 엔트로피 오차 손실 함수를 구한다\n",
    "    def loss(self, x, t):\n",
    "        # x : 입력 데이터, t : 정답 레이블\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    \n",
    "    # 정확도 구하기\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # 오차역전파법으로 기울기 구하기 -> 5장 학습 후 다시 보기\n",
    "    def gradient(self, x, t):\n",
    "        # 순전파 ~~~~~\n",
    "        a, z = {}, {}\n",
    "    \n",
    "        a[\"a1\"] = np.dot(x, self.params['W1']) + self.params['b1']\n",
    "        z[\"z1\"] = sigmoid(a[\"a1\"])\n",
    "        \n",
    "        for i in range(1, self.maxIndex - 1):\n",
    "            a[\"a\"+str(i+1)] = np.dot(z[\"z\"+str(i)], self.params['W'+str(i+1)]) + self.params['b'+str(i+1)]\n",
    "            z[\"z\"+str(i+1)] = sigmoid(a[\"a\"+str(i+1)])\n",
    "        \n",
    "        a[\"a\"+str(self.maxIndex)] = np.dot(z[\"z\"+str(self.maxIndex-1)], self.params['W'+str(self.maxIndex)]) + self.params['b'+str(self.maxIndex)]\n",
    "        y = softmax(a[\"a\"+str(self.maxIndex)])\n",
    "        \n",
    "        # 역전파 ~~~~~\n",
    "        grads = {}\n",
    "           \n",
    "        batch_num = x.shape[0]\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W' + str(self.maxIndex)] = np.dot(z[\"z\" + str(self.maxIndex - 1)].T, dy)\n",
    "        grads['b' + str(self.maxIndex)] = np.sum(dy, axis=0)\n",
    "        \n",
    "        dz = dy\n",
    "        for i in range(self.maxIndex-1, 1, -1):\n",
    "            da = np.dot(dz, grads[\"W\" + str(i + 1)].T)\n",
    "            dz = sigmoid_grad(a[\"a\" + str(i)]) * da\n",
    "            grads['W' + str(i)] = np.dot(z[\"z\" + str(i - 1)].T, dz)\n",
    "            grads['b' + str(i)] = np.sum(dz, axis=0)\n",
    "        \n",
    "        da = np.dot(dz, grads[\"W2\"].T)\n",
    "        dz = sigmoid_grad(a[\"a1\"]) * da\n",
    "        grads['W1'] = np.dot(x.T, dz)\n",
    "        grads['b1'] = np.sum(dz, axis=0)\n",
    "           \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa636506-3780-4e47-a8ca-d19ae0f6a862",
   "metadata": {},
   "source": [
    "## 미니배치 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "445a31d7-5426-4356-b7d5-fd1dbb1dfdae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "b1\n",
      "W2\n",
      "b2\n",
      "W3\n",
      "b3\n",
      "W4\n",
      "b4\n",
      "W5\n",
      "b5\n",
      "SGD 생성\n",
      "Momentum 생성\n",
      "AdaGrad 생성\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 과제 조건 변수들(반복횟수, 학습률, 은닉층 노드 수)\n",
    "hidden_size = [15, 15, 15, 15, 15]\n",
    "iters_num = 10000  \n",
    "learning_rate = 0.001\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 1000\n",
    "\n",
    "# 은닉층 수 = len(hidden_size)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=[15, 15, 15, 15, 15], output_size=10)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size // batch_size, 1)\n",
    "total = 0\n",
    "\n",
    "sgd = SGD()\n",
    "momentum = Momentum()\n",
    "ada = AdaGrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a69210e8-d5e1-46c6-b74b-76d5c3adc19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번 ---- train acc, test acc | 0.09863333333333334, 0.0958\n",
      "2번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "3번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "4번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "5번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "6번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "7번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "8번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "9번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "10번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "11번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "12번 ---- train acc, test acc | 0.10298333333333333, 0.1009\n",
      "13번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "14번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "15번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "16번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "17번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "18번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "19번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "20번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "21번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "22번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "23번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "24번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "25번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "26번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "27번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "28번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "29번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "30번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "31번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "32번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "33번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "34번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "35번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "36번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "37번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "38번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "39번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "40번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "41번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "42번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "43번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "44번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "45번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "46번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "47번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "48번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "49번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "50번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "51번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "52번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "53번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "54번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "55번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "56번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "57번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "58번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "59번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "60번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "61번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "62번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "63번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "64번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "65번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "66번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "67번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "68번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "69번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "70번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "71번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "72번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "73번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "74번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "75번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "76번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "77번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "78번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "79번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "80번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "81번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "82번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "83번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "84번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "85번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "86번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "87번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "88번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "89번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "90번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "91번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "92번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "93번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "94번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "95번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "96번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "97번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "98번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "99번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "100번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "101번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "102번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "103번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "104번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "105번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "106번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "107번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "108번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "109번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "110번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "111번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "112번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "113번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "114번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "115번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "116번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "117번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "118번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "119번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "120번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "121번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "122번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "123번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "124번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "125번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "126번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "127번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "128번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "129번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "130번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "131번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "132번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "133번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "134번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "135번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "136번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "137번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "138번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "139번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "140번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "141번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "142번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "143번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "144번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "145번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "146번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "147번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "148번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "149번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "150번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "151번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "152번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "153번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "154번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "155번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "156번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "157번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "158번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "159번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "160번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "161번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "162번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "163번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "164번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "165번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "166번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n",
      "167번 ---- train acc, test acc | 0.11236666666666667, 0.1135\n"
     ]
    }
   ],
   "source": [
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # SGD 학습, 모멘텀 학습, AdaGrad 학습\n",
    "    #sgd.update(network.params, grad)\n",
    "    momentum.update(network.params, grad)\n",
    "    #ada.update(network.params, grad)\n",
    "    \n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        total += 1\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(str(total) + \"번\" + \" ---- \" + \"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5414f73-02d7-4035-905b-0bcac8d176af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b5f68-fded-4e55-a8c4-e77b20a93d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7997de5d",
   "metadata": {},
   "source": [
    "# 아래는 기울기 소실이 발생하는 것으로 추측\n",
    "# 기울기 소실이 아닌 가중치 초기값 설정 문제\n",
    "# -> Xavier 초기값 적용 후 해보기로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d8e8c",
   "metadata": {},
   "source": [
    "```python\n",
    "class TwoLayerNet:\n",
    "\n",
    "    # 가중치 초기화\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.maxIndex = len(hidden_size)  # 입력층, 출력층 포함\n",
    "\n",
    "        # 첫 번째 층\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size[0])\n",
    "        self.params['b1'] = np.zeros(hidden_size[0])\n",
    "\n",
    "        # 중간 층\n",
    "        for i in range(1, self.maxIndex - 1):\n",
    "            self.params['W' + str(i + 1)] = weight_init_std * np.random.randn(hidden_size[i - 1], hidden_size[i])\n",
    "            self.params['b' + str(i + 1)] = np.zeros(hidden_size[i])\n",
    "\n",
    "        # 마지막 층\n",
    "        self.params['W' + str(self.maxIndex)] = weight_init_std * np.random.randn(hidden_size[-1], output_size)\n",
    "        self.params['b' + str(self.maxIndex)] = np.zeros(output_size)\n",
    "\n",
    "        # 모든 가중치 및 편향 출력 확인\n",
    "        for key in self.params.keys():\n",
    "            print(key)\n",
    "        \n",
    "        \n",
    "    # 가중치와 편향으로 계산 신경망 계산\n",
    "    def predict(self, x):\n",
    "        a, z = {}, {}\n",
    "    \n",
    "        a[\"a1\"] = np.dot(x, self.params['W1']) + self.params['b1']\n",
    "        z[\"z1\"] = sigmoid(a[\"a1\"])\n",
    "        \n",
    "        for i in range(1, self.maxIndex - 1):\n",
    "            a[\"a\"+str(i+1)] = np.dot(z[\"z\"+str(i)], self.params['W'+str(i+1)]) + self.params['b'+str(i+1)]\n",
    "            z[\"z\"+str(i+1)] = sigmoid(a[\"a\"+str(i+1)])\n",
    "        \n",
    "        a[\"a\"+str(self.maxIndex)] = np.dot(z[\"z\"+str(self.maxIndex-1)], self.params['W'+str(self.maxIndex)]) + self.params['b'+str(self.maxIndex)]\n",
    "        y = softmax(a[\"a\"+str(self.maxIndex)])\n",
    "        \n",
    "        return y        \n",
    "        \n",
    "    # 교차 엔트로피 오차 손실 함수를 구한다\n",
    "    def loss(self, x, t):\n",
    "        # x : 입력 데이터, t : 정답 레이블\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    \n",
    "    # 정확도 구하기\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # 오차역전파법으로 기울기 구하기 -> 5장 학습 후 다시 보기\n",
    "    def gradient(self, x, t):\n",
    "        # 순전파 ~~~~~\n",
    "        a, z = {}, {}\n",
    "    \n",
    "        a[\"a1\"] = np.dot(x, self.params['W1']) + self.params['b1']\n",
    "        z[\"z1\"] = sigmoid(a[\"a1\"])\n",
    "        \n",
    "        for i in range(1, self.maxIndex - 1):\n",
    "            a[\"a\"+str(i+1)] = np.dot(z[\"z\"+str(i)], self.params['W'+str(i+1)]) + self.params['b'+str(i+1)]\n",
    "            z[\"z\"+str(i+1)] = sigmoid(a[\"a\"+str(i+1)])\n",
    "        \n",
    "        a[\"a\"+str(self.maxIndex)] = np.dot(z[\"z\"+str(self.maxIndex-1)], self.params['W'+str(self.maxIndex)]) + self.params['b'+str(self.maxIndex)]\n",
    "        y = softmax(a[\"a\"+str(self.maxIndex)])\n",
    "        \n",
    "        # 역전파 ~~~~~\n",
    "        grads = {}\n",
    "           \n",
    "        # 소프트맥스 \n",
    "        batch_num = x.shape[0]\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W' + str(self.maxIndex)] = np.dot(z[\"z\" + str(self.maxIndex - 1)].T, dy)\n",
    "        grads['b' + str(self.maxIndex)] = np.sum(dy, axis=0)\n",
    "        \n",
    "        dz = dy\n",
    "        for i in range(self.maxIndex-1, 1, -1):\n",
    "            da = np.dot(dz, grads[\"W\" + str(i + 1)].T)\n",
    "            dz = sigmoid_grad(a[\"a\" + str(i)]) * da\n",
    "            grads['W' + str(i)] = np.dot(z[\"z\" + str(i - 1)].T, dz)\n",
    "            grads['b' + str(i)] = np.sum(dz, axis=0)\n",
    "        \n",
    "        # 시그모이드\n",
    "        da = np.dot(dz, grads[\"W2\"].T)\n",
    "        dz = sigmoid_grad(a[\"a1\"]) * da\n",
    "        grads['W1'] = np.dot(x.T, dz)\n",
    "        grads['b1'] = np.sum(dz, axis=0)\n",
    "        \n",
    "    \n",
    "        # 시그모이드\n",
    "        #grads['W3'] = np.dot(z[\"z2\"].T, dy)\n",
    "        #grads['b3'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        # 시그모이드\n",
    "        #da2 = np.dot(dy, grads[\"W3\"].T)\n",
    "        #dz2 = sigmoid_grad(a[\"a2\"]) * da2\n",
    "        #grads['W2'] = np.dot(z[\"z1\"].T, dz2)\n",
    "        #grads['b2'] = np.sum(dz2, axis=0)\n",
    "        \n",
    "        # 시그모이드\n",
    "        #da1 = np.dot(dz2, grads[\"W2\"].T)\n",
    "        #dz1 = sigmoid_grad(a[\"a1\"]) * da1\n",
    "        #grads['W1'] = np.dot(x.T, dz1)\n",
    "        #grads['b1'] = np.sum(dz1, axis=0)\n",
    "           \n",
    "        return grads\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de95318f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3547f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febafb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3460f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d27ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa16c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8d3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601df5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1ccf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43c1de-75b7-4fdb-b4f2-194cbe60cbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
